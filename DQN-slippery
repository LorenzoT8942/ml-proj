import gymnasium as gym
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import random
from collections import deque
import matplotlib.pyplot as plt
from tqdm import tqdm

# Check if CUDA is available and set device accordingly
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# Define the neural network for the DQN
class DQN(nn.Module):
    def __init__(self, input_dim, output_dim):
        """
        Initialize the neural network for the Deep Q-Network.
        
        Args:
            input_dim: Size of the input state
            output_dim: Number of possible actions
        """
        super(DQN, self).__init__()
        self.fc1 = nn.Linear(input_dim, 128)
        self.fc2 = nn.Linear(128, 128)
        self.fc3 = nn.Linear(128, output_dim)
    
    def forward(self, x):
        """
        Forward pass through the network.
        
        Args:
            x: Input tensor representing the state
            
        Returns:
            Q-values for each possible action
        """
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        return self.fc3(x)

# Implement the replay buffer to store experiences
class ReplayBuffer:
    def __init__(self, capacity):
        """
        Initialize the replay buffer with a fixed capacity.
        
        Args:
            capacity: Maximum number of transitions to store
        """
        self.buffer = deque(maxlen=capacity)
    
    def push(self, state, action, reward, next_state, done):
        """
        Add a transition to the buffer.
        
        Args:
            state: Current state
            action: Action taken
            reward: Reward received
            next_state: Next state
            done: Flag indicating if the episode has ended
        """
        self.buffer.append((state, action, reward, next_state, done))
    
    def sample(self, batch_size):
        """
        Sample a batch of transitions from the buffer.
        
        Args:
            batch_size: Size of the batch to sample
            
        Returns:
            Batch of transitions as tuples of tensors
        """
        states, actions, rewards, next_states, dones = zip(*random.sample(self.buffer, batch_size))
        
        # Convert all components to PyTorch tensors and move to the appropriate device
        states = torch.tensor(np.array(states), dtype=torch.float32).to(device)
        actions = torch.tensor(np.array(actions), dtype=torch.long).to(device)
        rewards = torch.tensor(np.array(rewards), dtype=torch.float32).to(device)
        next_states = torch.tensor(np.array(next_states), dtype=torch.float32).to(device)
        dones = torch.tensor(np.array(dones), dtype=torch.float32).to(device)
        
        return states, actions, rewards, next_states, dones
    
    def __len__(self):
        """
        Returns:
            Current number of transitions in the buffer
        """
        return len(self.buffer)

# Single Network DQN Agent class
class SingleNetworkDQNAgent:
    def __init__(self, state_size, action_size, learning_rate=0.001, gamma=0.99, 
                 epsilon_start=1.0, epsilon_end=0.01, epsilon_decay=0.995, 
                 buffer_size=20000, batch_size=64):
        """
        Initialize the DQN agent with a single network.
        
        Args:
            state_size: Size of the state space
            action_size: Size of the action space
            learning_rate: Learning rate
            gamma: Discount factor for future rewards
            epsilon_start: Initial value of epsilon for exploration
            epsilon_end: Minimum value of epsilon
            epsilon_decay: Decay factor for epsilon
            buffer_size: Size of the replay buffer
            batch_size: Batch size for learning
        """
        self.state_size = state_size
        self.action_size = action_size
        self.gamma = gamma
        self.epsilon = epsilon_start
        self.epsilon_end = epsilon_end
        self.epsilon_decay = epsilon_decay
        self.batch_size = batch_size
        self.learning_counter = 0
        
        # Initialize only one network
        self.network = DQN(state_size, action_size).to(device)
        
        self.optimizer = optim.Adam(self.network.parameters(), lr=learning_rate)
        self.memory = ReplayBuffer(buffer_size)
    
    def select_action(self, state):
        """
        Select an action using an epsilon-greedy policy.
        
        Args:
            state: Current state
            
        Returns:
            Selected action
        """
        if random.random() > self.epsilon:
            with torch.no_grad():
                state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)
                q_values = self.network(state_tensor)
                return torch.argmax(q_values).item()
        else:
            return random.randint(0, self.action_size - 1)
    
    def learn(self):
        """
        Update the network using a batch of experiences from the buffer.
        """
        if len(self.memory) < self.batch_size:
            return
        
        states, actions, rewards, next_states, dones = self.memory.sample(self.batch_size)
        
        # Calculate current Q-values (Q(s,a) for the action taken)
        q_values = self.network(states).gather(1, actions.unsqueeze(1)).squeeze(1)
        
        # Calculate next Q-values (max Q-value for the next state)
        with torch.no_grad():
            next_q_values = self.network(next_states).max(1)[0]
            expected_q_values = rewards + self.gamma * next_q_values * (1 - dones)
        
        # Calculate the loss (Huber loss for more robustness)
        #loss = F.smooth_l1_loss(q_values, expected_q_values)
        loss = F.mse_loss(q_values, expected_q_values)
        
        # Optimization
        self.optimizer.zero_grad()
        loss.backward()
        # Clip gradient for stability
        for param in self.network.parameters():
            param.grad.data.clamp_(-1, 1)
        self.optimizer.step()
        
        # Decay epsilon
        self.epsilon = max(self.epsilon_end, self.epsilon * self.epsilon_decay)
        
        self.learning_counter += 1
        
        return loss.item()
    
    def save_model(self, path):
        """
        Save the model.
        
        Args:
            path: Path to save the model
        """
        torch.save({
            'network_state_dict': self.network.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'epsilon': self.epsilon
        }, path)
    
    def load_model(self, path):
        """
        Load the model.
        
        Args:
            path: Path to load the model from
        """
        checkpoint = torch.load(path)
        self.network.load_state_dict(checkpoint['network_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        self.epsilon = checkpoint['epsilon']

# One-hot encoding function for the state
def one_hot_encode(state, num_states):
    """
    Encode a discrete state as a one-hot vector.
    
    Args:
        state: State index
        num_states: Total number of possible states
        
    Returns:
        One-hot vector representing the state
    """
    encoded = np.zeros(num_states)
    encoded[state] = 1
    return encoded

# Main training function
def train_agent(env_name, num_episodes=3000, render_freq=100, is_slippery=True):
    """
    Train the DQN agent on the specified environment.
    
    Args:
        env_name: Name of the Gymnasium environment
        num_episodes: Number of training episodes
        render_freq: How often to display training progress
        is_slippery: Whether to make the environment non-deterministic
        
    Returns:
        Trained agent and reward history
    """
    # Create the environment with is_slippery=True for non-deterministic behavior
    env = gym.make(env_name, is_slippery=is_slippery)
    
    # For CliffWalking: the state is an integer representing the position in a 4x12 grid
    num_states = env.observation_space.n  # 48 states for CliffWalking
    num_actions = env.action_space.n  # 4 actions: left (0), down (1), right (2), up (3)
    
    agent = SingleNetworkDQNAgent(
        state_size=num_states, 
        action_size=num_actions,
        buffer_size=20000,  # Larger buffer for stochastic environments
        batch_size=128,     # Larger batches to reduce variance
        gamma=0.99,         # High discount factor to consider future rewards
        epsilon_decay=0.997, # Slower decay for more exploration
    )
    
    rewards_history = []
    avg_rewards_history = []
    losses = []
    
    # Structure to track performance statistics
    stats = {
        'success_rate': [],
        'avg_steps': [],
        'episodes_completed': 0
    }
    
    max_steps_per_episode = 200  # Limit the number of steps to avoid infinite loops
    
    for episode in tqdm(range(num_episodes)):
        state, _ = env.reset()
        state_encoded = one_hot_encode(state, num_states)
        done = False
        truncated = False
        total_reward = 0
        episode_losses = []
        steps = 0
        
        while not (done or truncated) and steps < max_steps_per_episode:
            action = agent.select_action(state_encoded)
            next_state, reward, done, truncated, _ = env.step(action)
            next_state_encoded = one_hot_encode(next_state, num_states)
            
            # Store the experience
            agent.memory.push(state_encoded, action, reward, next_state_encoded, done)
            
            # Update the network
            if len(agent.memory) > agent.batch_size:
                loss = agent.learn()
                if loss is not None:
                    episode_losses.append(loss)
            
            state = next_state
            state_encoded = next_state_encoded
            total_reward += reward
            steps += 1
        
        rewards_history.append(total_reward)
        avg_reward = np.mean(rewards_history[-100:])
        avg_rewards_history.append(avg_reward)
        
        # Update statistics
        if done and not truncated:  # Episode concluded by reaching the goal
            stats['episodes_completed'] += 1
        
        if episode % 100 == 0:
            success_rate = stats['episodes_completed'] / (episode + 1) if episode > 0 else 0
            stats['success_rate'].append(success_rate)
            stats['avg_steps'].append(np.mean([s for s in [steps] if s < max_steps_per_episode]))
        
        if episode_losses:
            avg_loss = np.mean(episode_losses)
            losses.append(avg_loss)
        
        if (episode + 1) % render_freq == 0:
            print(f"Episode {episode + 1}/{num_episodes}, Reward: {total_reward}, Avg reward: {avg_reward:.2f}, Epsilon: {agent.epsilon:.2f}")
            print(f"Success rate: {stats['episodes_completed'] / (episode + 1):.2%}")

    # Visualize the results
    plt.figure(figsize=(15, 10))
    
    plt.subplot(2, 2, 1)
    plt.plot(rewards_history, alpha=0.6, label='Reward per episode')
    plt.plot(avg_rewards_history, label='Moving average (100 episodes)')
    plt.xlabel('Episode')
    plt.ylabel('Reward')
    plt.title('Rewards during training')
    plt.legend()
    
    plt.subplot(2, 2, 2)
    if losses:
        plt.plot(losses)
        plt.xlabel('Learning iteration')
        plt.ylabel('Loss')
        plt.title('Loss during training')
    
    plt.subplot(2, 2, 3)
    plt.plot(range(0, num_episodes, 100), stats['success_rate'])
    plt.xlabel('Episode')
    plt.ylabel('Success rate')
    plt.title('Episode completion rate')
    
    plt.subplot(2, 2, 4)
    plt.plot(range(0, num_episodes, 100), stats['avg_steps'])
    plt.xlabel('Episode')
    plt.ylabel('Average steps')
    plt.title('Average steps per completed episode')
    
    plt.tight_layout()
    plt.savefig(f"single_network_dqn_{env_name}_slippery_{is_slippery}.png")
    plt.show()
    
    # Save the trained model
    agent.save_model(f"single_network_dqn_{env_name}_slippery_{is_slippery}.pth")
    
    return agent, rewards_history

# Function to test the trained agent
def test_agent(agent, env_name, num_episodes=20, render=True, is_slippery=True):
    """
    Test the agent on the specified environment.
    
    Args:
        agent: Trained DQN agent
        env_name: Name of the Gymnasium environment
        num_episodes: Number of test episodes
        render: Whether to render the environment
        is_slippery: Whether to make the environment non-deterministic
        
    Returns:
        List of rewards and statistics
    """
    env = gym.make(env_name, render_mode="human" if render else None, is_slippery=is_slippery)
    num_states = env.observation_space.n
    
    rewards = []
    steps_list = []
    success_count = 0
    
    for episode in range(num_episodes):
        state, _ = env.reset()
        state_encoded = one_hot_encode(state, num_states)
        done = False
        truncated = False
        total_reward = 0
        steps = 0
        
        while not (done or truncated) and steps < 200:  # Step limit to avoid infinite loops
            # During testing, we mostly use the best action (very low epsilon)
            if random.random() > 0.05:  # 5% residual exploration for the slippery version
                with torch.no_grad():
                    state_tensor = torch.tensor(state_encoded, dtype=torch.float32).unsqueeze(0).to(device)
                    q_values = agent.network(state_tensor)
                    action = torch.argmax(q_values).item()
            else:
                action = random.randint(0, agent.action_size - 1)
                
            next_state, reward, done, truncated, _ = env.step(action)
            next_state_encoded = one_hot_encode(next_state, num_states)
            
            state = next_state
            state_encoded = next_state_encoded
            total_reward += reward
            steps += 1
        
        rewards.append(total_reward)
        steps_list.append(steps)
        
        if done and not truncated:  # Agent reached the goal
            success_count += 1
            
        print(f"Episode {episode + 1}, Reward: {total_reward}, Steps: {steps}")
    
    success_rate = success_count / num_episodes
    print(f"\nSuccess rate: {success_rate:.2%}")
    print(f"Average steps: {np.mean(steps_list):.1f}")
    print(f"Average reward: {np.mean(rewards):.1f}")
    
    env.close()
    return rewards, {"success_rate": success_rate, "avg_steps": np.mean(steps_list)}

# Function to visualize the learned Q-values
def visualize_q_values(agent, env_name, is_slippery=True):
    """
    Visualize the Q-values learned by the agent to understand the learned policy.
    
    Args:
        agent: Trained DQN agent
        env_name: Name of the environment
        is_slippery: Whether the environment is slippery or not
    """
    env = gym.make(env_name, is_slippery=is_slippery)
    num_states = env.observation_space.n
    
    # CliffWalking is a 4x12 grid
    height = 4
    width = 12
    
    # Calculate Q-values for each state
    q_values = np.zeros((num_states, agent.action_size))
    for state in range(num_states):
        state_encoded = one_hot_encode(state, num_states)
        state_tensor = torch.tensor(state_encoded, dtype=torch.float32).unsqueeze(0).to(device)
        with torch.no_grad():
            q_values[state] = agent.network(state_tensor).cpu().numpy()
    
    # Prepare the best actions map
    best_actions = np.argmax(q_values, axis=1)
    
    # Convert action indices to symbols
    action_symbols = ['←', '↓', '→', '↑']
    
    # Create a grid representation
    grid = []
    for i in range(height):
        row = []
        for j in range(width):
            state_idx = i * width + j
            row.append(action_symbols[best_actions[state_idx]])
        grid.append(row)
    
    # Identify special states (start, goal, cliff)
    # Start is at (3, 0), Goal is at (3, 11), cliff is between (3, 1) and (3, 10)
    for j in range(1, 11):
        grid[3][j] = 'C'  # Cliff
    grid[3][0] = 'S'  # Start
    grid[3][11] = 'G'  # Goal
    
    # Visualize the grid with best actions
    plt.figure(figsize=(15, 6))
    plt.subplot(1, 2, 1)
    q_map = plt.imshow(np.max(q_values.reshape(height, width, -1), axis=2), cmap='viridis')
    plt.colorbar(q_map, label='Max Q-Value')
    plt.title('Map of maximum Q-values')
    
    plt.subplot(1, 2, 2)
    table = plt.table(cellText=grid, loc='center', cellLoc='center')
    table.auto_set_font_size(False)
    table.set_fontsize(12)
    table.scale(1, 2)
    plt.axis('off')
    plt.title('Map of best actions')
    
    plt.tight_layout()
    plt.savefig(f"q_values_single_network_{env_name}_slippery_{is_slippery}.png")
    plt.show()

# Main function
if __name__ == "__main__":
    env_name = "CliffWalking-v0"
    is_slippery = True  # Set to True for the non-deterministic version
    
    # Train the agent
    print(f"Starting agent training on {'slippery' if is_slippery else 'deterministic'} environment...")
    agent, rewards = train_agent(env_name, num_episodes=3000, is_slippery=is_slippery)
    
    # Visualize the learned Q-values
    visualize_q_values(agent, env_name, is_slippery)
    
    # Test the agent
    print("\nTesting the trained agent:")
    test_rewards, stats = test_agent(agent, env_name, is_slippery=is_slippery)
    
    print(f"\nFinal results:")
    print(f"Success rate: {stats['success_rate']:.2%}")
    print(f"Average steps: {stats['avg_steps']:.1f}")